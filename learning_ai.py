import google.generativeai as genai
import os
import streamlit as st
from dotenv import load_dotenv


load_dotenv()

# Title
st.title("ğŸ“‘ Writing Analysis AI")

#Language Select
language = st.selectbox("Select Response LanguageğŸŒ", ["English", "Chinese", "Malay"])

#Temperature
temperature = st.slider("Creative RangeğŸ’¡", min_value=0.1, max_value=2.0, value=1.0, step=0.1)



# Config API Key
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])



#Create Model
model = genai.GenerativeModel("gemini-1.5-flash",
                              system_instruction=f"""è¯·ç”¨{language}è¯­è¨€æ¥å›ç­”æ‰€æœ‰é—®é¢˜ã€‚
                              å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
 1. è¯­æ³•ä¸æ‹¼å†™æ£€æŸ¥ï¼š
     - åˆ†æç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼Œå¹¶æ ‡è®°æ‰€æœ‰è¯­æ³•ã€æ‹¼å†™å’Œæ ‡ç‚¹é”™è¯¯ã€‚
     - å°è¯•çº æ­£è¿™äº›é”™è¯¯ï¼Œå¹¶æä¾›ä¿®æ”¹åçš„æ–‡æœ¬ã€‚
     - ä¸ºæ–‡æœ¬çš„è¯­æ³•å’Œæ‹¼å†™è´¨é‡è¯„åˆ†ï¼Œè¯„åˆ†èŒƒå›´ä¸º 0 åˆ° 10 åˆ†ã€‚
 2. ç»“æ„å’Œé€»è¾‘å»ºè®®ï¼š
     - åˆ†æç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬çš„ç»“æ„å’Œé€»è¾‘æµã€‚
     - æä¾›å»ºè®®ä»¥å¸®åŠ©ç”¨æˆ·æ”¹å–„æ®µè½ç»“æ„å’Œé€»è¾‘æµï¼Œè®©å†™ä½œæ›´è¿è´¯ã€‚
     - ä¸ºæ–‡æœ¬çš„ç»“æ„å’Œé€»è¾‘è´¨é‡è¯„åˆ†ï¼Œè¯„åˆ†èŒƒå›´ä¸º 0 åˆ° 10 åˆ†ã€‚
     æä¾›ä¼˜åŒ–å’Œä¿®æ”¹åçš„å®Œæ•´æ–‡æœ¬ï¼š""",
                              )



def main():
    

    # Content Generate
    def generate_content(query, temperature, language):
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=8192,
            top_p=0.95,
            top_k=40,
        )
        
        response = model.generate_content(
            query,
            generation_config=generation_config
        )
        
        return response.text if hasattr(response, 'text') else "No response generated."



    # Initialize History
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "Hello! I'm here to enhance your writing with grammar, spelling, and structure tips. Ask me anything!"
            }
        ]



    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])



    # Process and store Query and Response
    def llm_function(query):
        response = generate_content(query, temperature, language)

        
        # Displaying the Assistant Message
        with st.chat_message("assistant"):
            st.markdown(response)


        # Storing the User Message
        st.session_state.messages.append(
            {
                "role":"user",
                "content": query
            }
        )


        # Storing the Assistant Message
        st.session_state.messages.append(
            {
                "role":"assistant",
                "content": response
            }
        )

    


    # User Input
    query = st.chat_input("How may I help you?")
     


    # Process User Input
    # Calling the Function when Input is Provided
    if query:
        # Displaying the User Message
        with st.chat_message("user"):
            st.markdown(query)
        
        
        # å°†æ‰€æœ‰å†å²ä¿¡æ¯æ‹¼æ¥èµ·æ¥
        full_query = "\n".join([message["content"] for message in st.session_state.messages])
        full_query += f"\n{query}"
        llm_function(full_query)

        # llm_function(query)

        

if __name__ == "__main__":
    main()
